{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data mining project - 2020/21</b><br>\n",
    "<b>Author</b>: [Alexandra Bradan](https://github.com/alexandrabradan)<br>\n",
    "<b>Python version</b>: 3.x<br>\n",
    "<b>Last update: 07/01/2021<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# general libraries\n",
    "import sys\n",
    "import math\n",
    "import operator\n",
    "import itertools\n",
    "import pydotplus\n",
    "import collections\n",
    "import missingno as msno\n",
    "from pylab import MaxNLocator\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "\n",
    "from collections import defaultdict\n",
    "from IPython.display import Image\n",
    "\n",
    "# pandas libraries\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "# visualisation libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "# numpy libraries\n",
    "import numpy as np\n",
    "from numpy import std\n",
    "from numpy import mean\n",
    "from numpy import arange\n",
    "from numpy import unique\n",
    "from numpy import argmax\n",
    "from numpy import percentile\n",
    "\n",
    "# scipy libraries\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import kstest\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "# sklearn libraries\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.experimental import enable_iterative_imputer  # explicitly require this experimental feature\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from imblearn.pipeline import make_pipeline as imbmake_pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, fbeta_score, recall_score, precision_score, classification_report, roc_auc_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fim import apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../../../data/\"\n",
    "TR_impunted_file = data_directory + \"Impunted_Train_HR_Employee_Attrition.csv\"\n",
    "TS_impunted_file = data_directory + \"Impunted_Test_HR_Employee_Attrition.csv\"\n",
    "TR_not_impunted = data_directory + \"Not_Impunted_Train_HR_Employee_Attrition.csv\"\n",
    "TS_not_impunted = data_directory + \"Cleaned_Test_HR_Employee_Attrition.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Impunted TR Dataframe</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(883, 30)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_impunted = pd.read_csv(TR_impunted_file, sep=\",\") \n",
    "df_impunted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(883, 24)\n"
     ]
    }
   ],
   "source": [
    "to_drop = [\"MaritalStatus\", \"EducationField\", \"Department\", \"YearsSinceLastPromotion\", \"HourlyRate\", \"MonthlyRate\"]\n",
    "\n",
    "# drop features \n",
    "for column_name in to_drop:\n",
    "    del df_impunted[column_name]\n",
    "    \n",
    "# check dropping output\n",
    "print(df_impunted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 883 entries, 0 to 882\n",
      "Data columns (total 24 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Age                       883 non-null    int64  \n",
      " 1   Attrition                 883 non-null    object \n",
      " 2   BusinessTravel            883 non-null    object \n",
      " 3   DistanceFromHome          883 non-null    int64  \n",
      " 4   Education                 883 non-null    int64  \n",
      " 5   EnvironmentSatisfaction   883 non-null    int64  \n",
      " 6   Gender                    883 non-null    object \n",
      " 7   JobInvolvement            883 non-null    int64  \n",
      " 8   JobLevel                  883 non-null    int64  \n",
      " 9   JobRole                   883 non-null    object \n",
      " 10  JobSatisfaction           883 non-null    int64  \n",
      " 11  MonthlyIncome             883 non-null    int64  \n",
      " 12  NumCompaniesWorked        883 non-null    int64  \n",
      " 13  OverTime                  883 non-null    object \n",
      " 14  PercentSalaryHike         883 non-null    int64  \n",
      " 15  RelationshipSatisfaction  883 non-null    int64  \n",
      " 16  StockOptionLevel          883 non-null    int64  \n",
      " 17  TrainingTimesLastYear     883 non-null    int64  \n",
      " 18  WorkLifeBalance           883 non-null    int64  \n",
      " 19  YearsAtCompany            883 non-null    int64  \n",
      " 20  YearsInCurrentRole        883 non-null    int64  \n",
      " 21  MonthlyHours              883 non-null    float64\n",
      " 22  TaxRate                   883 non-null    float64\n",
      " 23  OverallSatisfaction       883 non-null    float64\n",
      "dtypes: float64(3), int64(16), object(5)\n",
      "memory usage: 165.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_impunted.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Not-impunted TR DataFrame</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(883, 30)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_not_impunted = pd.read_csv(TR_not_impunted, sep=\",\") \n",
    "df_not_impunted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(883, 24)\n"
     ]
    }
   ],
   "source": [
    "to_drop = [\"MaritalStatus\", \"EducationField\", \"Department\", \"YearsSinceLastPromotion\", \"HourlyRate\", \"MonthlyRate\"]\n",
    "\n",
    "# drop features \n",
    "for column_name in to_drop:\n",
    "    del df_not_impunted[column_name]\n",
    "    \n",
    "# check dropping output\n",
    "print(df_not_impunted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Impunted TS DataFrame </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219, 24)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ts = pd.read_csv(TS_impunted_file, sep=\",\") \n",
    "df_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 219 entries, 0 to 218\n",
      "Data columns (total 24 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Age                       219 non-null    int64  \n",
      " 1   Attrition                 219 non-null    object \n",
      " 2   BusinessTravel            219 non-null    object \n",
      " 3   DistanceFromHome          219 non-null    int64  \n",
      " 4   Education                 219 non-null    int64  \n",
      " 5   EnvironmentSatisfaction   219 non-null    int64  \n",
      " 6   Gender                    219 non-null    object \n",
      " 7   JobInvolvement            219 non-null    int64  \n",
      " 8   JobLevel                  219 non-null    int64  \n",
      " 9   JobRole                   219 non-null    object \n",
      " 10  JobSatisfaction           219 non-null    int64  \n",
      " 11  MonthlyIncome             219 non-null    int64  \n",
      " 12  NumCompaniesWorked        219 non-null    int64  \n",
      " 13  OverTime                  219 non-null    object \n",
      " 14  PercentSalaryHike         219 non-null    int64  \n",
      " 15  RelationshipSatisfaction  219 non-null    int64  \n",
      " 16  StockOptionLevel          219 non-null    int64  \n",
      " 17  TrainingTimesLastYear     219 non-null    int64  \n",
      " 18  WorkLifeBalance           219 non-null    int64  \n",
      " 19  YearsAtCompany            219 non-null    int64  \n",
      " 20  YearsInCurrentRole        219 non-null    int64  \n",
      " 21  MonthlyHours              219 non-null    float64\n",
      " 22  TaxRate                   219 non-null    float64\n",
      " 23  OverallSatisfaction       219 non-null    float64\n",
      "dtypes: float64(3), int64(16), object(5)\n",
      "memory usage: 41.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_ts.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Not-impnted TS DataFrame </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped rows = \t8\n",
      "dropped rows = \t2\n",
      "dropped rows = \t7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(219, 24)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ts_not_impunted = pd.read_csv(TS_not_impunted, sep=\",\") \n",
    "df_ts_not_impunted.shape\n",
    "\n",
    "del df_ts_not_impunted[\"Department\"]\n",
    "del df_ts_not_impunted[\"MonthlyRate\"]\n",
    "\n",
    "\n",
    "to_drop_indexes = df_ts_not_impunted.index[df_ts_not_impunted[\"YearsAtCompany\"] > 20]\n",
    "df_ts_not_impunted.drop(list(to_drop_indexes), axis=0, inplace=True)\n",
    "df_ts_not_impunted.reset_index(drop=True, inplace=True)\n",
    "print(\"dropped rows = \", len(to_drop_indexes), sep=\"\\t\")\n",
    "\n",
    "to_drop_indexes = df_ts_not_impunted.index[df_ts_not_impunted[\"YearsInCurrentRole\"] > 16]\n",
    "df_ts_not_impunted.drop(list(to_drop_indexes), axis=0, inplace=True)\n",
    "df_ts_not_impunted.reset_index(drop=True, inplace=True)\n",
    "print(\"dropped rows = \", len(to_drop_indexes), sep=\"\\t\")\n",
    "\n",
    "to_drop_indexes = df_ts_not_impunted.index[df_ts_not_impunted[\"MonthlyHours\"] > 590.9767441860465]\n",
    "df_ts_not_impunted.drop(list(to_drop_indexes), axis=0, inplace=True)\n",
    "df_ts_not_impunted.reset_index(drop=True, inplace=True)\n",
    "print(\"dropped rows = \", len(to_drop_indexes), sep=\"\\t\")\n",
    "\n",
    "df_ts_not_impunted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 219 entries, 0 to 218\n",
      "Data columns (total 24 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Age                       191 non-null    float64\n",
      " 1   Attrition                 219 non-null    object \n",
      " 2   BusinessTravel            202 non-null    object \n",
      " 3   DistanceFromHome          219 non-null    int64  \n",
      " 4   Education                 219 non-null    int64  \n",
      " 5   EnvironmentSatisfaction   219 non-null    int64  \n",
      " 6   Gender                    208 non-null    object \n",
      " 7   JobInvolvement            219 non-null    int64  \n",
      " 8   JobLevel                  219 non-null    int64  \n",
      " 9   JobRole                   219 non-null    object \n",
      " 10  JobSatisfaction           219 non-null    int64  \n",
      " 11  MonthlyIncome             168 non-null    float64\n",
      " 12  NumCompaniesWorked        219 non-null    int64  \n",
      " 13  OverTime                  219 non-null    object \n",
      " 14  PercentSalaryHike         219 non-null    int64  \n",
      " 15  RelationshipSatisfaction  219 non-null    int64  \n",
      " 16  StockOptionLevel          219 non-null    int64  \n",
      " 17  TrainingTimesLastYear     170 non-null    float64\n",
      " 18  WorkLifeBalance           219 non-null    int64  \n",
      " 19  YearsAtCompany            207 non-null    float64\n",
      " 20  YearsInCurrentRole        219 non-null    int64  \n",
      " 21  MonthlyHours              219 non-null    float64\n",
      " 22  TaxRate                   168 non-null    float64\n",
      " 23  OverallSatisfaction       219 non-null    float64\n",
      "dtypes: float64(7), int64(12), object(5)\n",
      "memory usage: 41.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_ts_not_impunted.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Continuos variables discretisation with K-Means</h2>\n",
    "K-Means => similar bins' size => IBM's Age and opportunity equity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_converted = df_impunted.copy()\n",
    "df_ts_conv = df_ts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_performed_encoding(column_name, train_encoded):\n",
    "    column_index = df_impunted.columns.get_loc(column_name)\n",
    "    encoding_info = {}\n",
    "    for enc, i in zip(train_encoded, range(0, len(train_encoded))):\n",
    "        try:\n",
    "            tmp_list = encoding_info[str(enc)]\n",
    "            tmp_list.append(df_impunted.iloc[i, column_index])\n",
    "            encoding_info[str(enc)] = tmp_list\n",
    "        except KeyError:\n",
    "            encoding_info[str(enc)] = [df_impunted.iloc[i, column_index]]\n",
    "            \n",
    "    for key, value in encoding_info.items():\n",
    "        min_value = min(value)\n",
    "        max_value = max(value)\n",
    "        print(column_name, key, \"[%s-%s]\" %(min_value, max_value),sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age\t[3]\t[48-60]\n",
      "Age\t[1]\t[31-38]\n",
      "Age\t[0]\t[18-30]\n",
      "Age\t[2]\t[39-47]\n",
      "DistanceFromHome\t[1]\t[6-13]\n",
      "DistanceFromHome\t[0]\t[1-5]\n",
      "DistanceFromHome\t[2]\t[14-21]\n",
      "DistanceFromHome\t[3]\t[22-29]\n",
      "MonthlyIncome\t[1]\t[7094-13888]\n",
      "MonthlyIncome\t[0]\t[1009-6992]\n",
      "MonthlyIncome\t[2]\t[14004-20520]\n",
      "MonthlyIncome\t[3]\t[20933-26997]\n",
      "NumCompaniesWorked\t[3]\t[7-9]\n",
      "NumCompaniesWorked\t[0]\t[0-2]\n",
      "NumCompaniesWorked\t[1]\t[3-4]\n",
      "NumCompaniesWorked\t[2]\t[5-6]\n",
      "PercentSalaryHike\t[1]\t[15-18]\n",
      "PercentSalaryHike\t[0]\t[11-14]\n",
      "PercentSalaryHike\t[3]\t[22-25]\n",
      "PercentSalaryHike\t[2]\t[19-21]\n",
      "YearsAtCompany\t[1]\t[6-10]\n",
      "YearsAtCompany\t[0]\t[0-5]\n",
      "YearsAtCompany\t[2]\t[11-14]\n",
      "YearsAtCompany\t[3]\t[15-20]\n",
      "YearsInCurrentRole\t[1]\t[5-8]\n",
      "YearsInCurrentRole\t[0]\t[0-4]\n",
      "YearsInCurrentRole\t[2]\t[9-12]\n",
      "YearsInCurrentRole\t[3]\t[13-16]\n",
      "TaxRate\t[1]\t[0.2033107599699022-0.4878233954330433]\n",
      "TaxRate\t[2]\t[0.4902649218001915-0.7143783124261257]\n",
      "TaxRate\t[3]\t[0.7168701095461659-0.9513959334891722]\n",
      "TaxRate\t[0]\t[0.0-0.20014044943820225]\n",
      "MonthlyHours\t[1]\t[151.64893617021278-264.325]\n",
      "MonthlyHours\t[3]\t[404.2033898305085-590.9767441860465]\n",
      "MonthlyHours\t[2]\t[265.0588235294117-403.135593220339]\n",
      "MonthlyHours\t[0]\t[26.04347826086957-150.85333333333332]\n",
      "OverallSatisfaction\t[2]\t[2.6-3.0]\n",
      "OverallSatisfaction\t[3]\t[3.2-4.0]\n",
      "OverallSatisfaction\t[1]\t[2.0-2.4]\n",
      "OverallSatisfaction\t[0]\t[1.2-1.8]\n"
     ]
    }
   ],
   "source": [
    "cont_variables = [\"Age\", \"DistanceFromHome\", \"MonthlyIncome\", \"NumCompaniesWorked\", \"PercentSalaryHike\",\n",
    "                 \"YearsAtCompany\", \"YearsInCurrentRole\", \"TaxRate\", \"MonthlyHours\", \"OverallSatisfaction\"]\n",
    "\n",
    "for column_name in cont_variables:\n",
    "    discretizer = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='kmeans').fit(df_converted[[column_name]])\n",
    "    train_encoded = discretizer.transform(df_converted[[column_name]]).astype(int)\n",
    "    df_converted[column_name] = train_encoded\n",
    "    df_ts_conv[column_name] = discretizer.transform(df_ts_conv[[column_name]]).astype(int)\n",
    "    print_performed_encoding(column_name, train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in list(df_converted.columns):\n",
    "    df_converted[column_name] = df_converted[column_name].astype(str) + \"_\" + column_name\n",
    "    df_ts_conv[column_name] = df_ts_conv[column_name].astype(str) + \"_\" + column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transaction creation \n",
    "df_db = df_converted.values.tolist()\n",
    "df_ts_db = df_ts_conv.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Missing values TR and TS discretisation </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not_impunted_converted = df_not_impunted.copy()\n",
    "df_ts_not_imp_conv = df_ts_not_impunted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_variables = [\"Age\", \"DistanceFromHome\", \"MonthlyIncome\", \"NumCompaniesWorked\", \"PercentSalaryHike\",\n",
    "                 \"YearsAtCompany\", \"YearsInCurrentRole\", \"TaxRate\", \"MonthlyHours\", \"OverallSatisfaction\"]\n",
    "\n",
    "for column_name in cont_variables:\n",
    "    discretizer = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='kmeans').fit(df_impunted[[column_name]])\n",
    "    train_encoded = discretizer.transform(df_impunted[[column_name]]).astype(int)\n",
    "    df_not_impunted_converted[column_name] = train_encoded\n",
    "    df_ts_not_imp_conv[column_name] = discretizer.transform(df_ts[[column_name]]).astype(int)\n",
    "    # print_performed_encoding(column_name, train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in list(df_converted.columns):\n",
    "    df_not_impunted_converted[column_name] = df_not_impunted_converted[column_name].astype(str) + \"_\" + column_name\n",
    "    df_ts_not_imp_conv[column_name] = df_ts_not_imp_conv[column_name].astype(str) + \"_\" + column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_numeric_values_columns = [\"Age\", \"YearsAtCompany\", \"MonthlyIncome\", \"TaxRate\", \"TrainingTimesLastYear\"]\n",
    "for column_name in missing_numeric_values_columns:\n",
    "    for i in range(0, len(df_not_impunted)):\n",
    "        column_index = df_not_impunted.columns.get_loc(column_name)\n",
    "        if df_not_impunted.iloc[i, column_index] >= 0:\n",
    "            pass\n",
    "        else:\n",
    "            df_not_impunted_converted.iloc[i, column_index] =  df_not_impunted.iloc[i, column_index]\n",
    "            \n",
    "for column_name in missing_numeric_values_columns:\n",
    "    for i in range(0, len(df_ts_not_impunted)):\n",
    "        column_index = df_ts_not_impunted.columns.get_loc(column_name)\n",
    "        if df_ts_not_impunted.iloc[i, column_index] >= 0:\n",
    "            pass\n",
    "        else:\n",
    "            df_ts_not_imp_conv.iloc[i, column_index] = df_ts_not_impunted.iloc[i, column_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_categorical_values_columns = [\"BusinessTravel\", \"Gender\"]\n",
    "for column_name in missing_categorical_values_columns:\n",
    "    for i in range(0, len(df_not_impunted)):\n",
    "        column_index = df_not_impunted.columns.get_loc(column_name)\n",
    "        if df_not_impunted.iloc[i, column_index] != \"Male\" and \\\n",
    "        df_not_impunted.iloc[i, column_index] != \"Female\" and \\\n",
    "        df_not_impunted.iloc[i, column_index] != \"Travel_Rarely\" and \\\n",
    "        df_not_impunted.iloc[i, column_index] != \"Travel_Frequently\" and \\\n",
    "        df_not_impunted.iloc[i, column_index] != \"Non-Travel\":\n",
    "            df_not_impunted_converted.iloc[i, column_index] =  df_not_impunted.iloc[i, column_index]\n",
    "            \n",
    "for column_name in missing_categorical_values_columns:\n",
    "    for i in range(0, len(df_ts_not_impunted)):\n",
    "        column_index = df_ts_not_impunted.columns.get_loc(column_name)\n",
    "        if df_ts_not_impunted.iloc[i, column_index] != \"Male\" and \\\n",
    "        df_ts_not_impunted.iloc[i, column_index] != \"Female\" and \\\n",
    "        df_ts_not_impunted.iloc[i, column_index] != \"Travel_Rarely\" and \\\n",
    "        df_ts_not_impunted.iloc[i, column_index] != \"Travel_Frequently\" and \\\n",
    "        df_ts_not_impunted.iloc[i, column_index] != \"Non-Travel\":\n",
    "            df_ts_not_imp_conv.iloc[i, column_index] = df_ts_not_impunted.iloc[i, column_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Generate \"Yes_Attrition\" frequent itemsets</h2>\n",
    "\n",
    "Since in our dataset employees leaving are 153/883 * 100 = 17.33 % we will search AR having:\n",
    "- support \\in [1, 18]\n",
    "- confidence \\in [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "\n",
    "and computing at the same time the lift.\n",
    "\n",
    "Remember that given an association rule X -> Y, where X is a k-itemset, with k=2,...,n_features and Y is a 1-itemset ({\"Yes_Attrtion\"} or {\"No_Attrition\"}):\n",
    "- **support(XuY) = support_count(XuY) / 883**;\n",
    "- **confidence(XuY) = support_count(XuY) / support_count(X)**;\n",
    "- **lift(XuY) = confidence(XuY) / support_count(Y)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(apriori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_presence_in_most_specific_frequent_itemset(most_specific_frequent_itemset, itemset1):\n",
    "    presence_flag = False\n",
    "    for key, value in most_specific_frequent_itemset.items():\n",
    "        replace_flag = False\n",
    "        itemset2 = value[\"itemset\"] \n",
    "        support_count2 = value[\"support_count\"] \n",
    "        support2 = value[\"support\"] \n",
    "        if set(itemset1[0]).issubset(set(itemset2)):\n",
    "            presence_flag = True\n",
    "            break\n",
    "            \"\"\"if len(set(itemset2).difference(set(itemset1[0]))) == 0:  # |itemset2| == |itemset1[0]|\n",
    "                if itemset1[1] > support_count2 and itemset1[2] > support2:\n",
    "                    replace_flag = True\n",
    "                    presence_flag = True\"\"\"\n",
    "        elif set(itemset2).issubset(set(itemset1[0])):\n",
    "            replace_flag = True\n",
    "        if replace_flag:\n",
    "            # replace itemset in dict with current one (more specific)\n",
    "            most_specific_frequent_itemset[str(key)][\"itemset\"] = itemset1[0]\n",
    "            most_specific_frequent_itemset[str(key)][\"support_count\"] = itemset1[1]\n",
    "            most_specific_frequent_itemset[str(key)][\"support\"] = itemset1[2]\n",
    "    if presence_flag is False:\n",
    "        # new itemset to insert in dict\n",
    "        last_key_index = len(most_specific_frequent_itemset)\n",
    "        most_specific_frequent_itemset[str(last_key_index)] = {}\n",
    "        most_specific_frequent_itemset[str(last_key_index)][\"itemset\"] = itemset1[0]\n",
    "        most_specific_frequent_itemset[str(last_key_index)][\"support_count\"] = itemset1[1]\n",
    "        most_specific_frequent_itemset[str(last_key_index)][\"support\"] = itemset1[2]\n",
    "    return most_specific_frequent_itemset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of itemsets for zmin=2, supp=1: 634394\n",
      "Number of itemsets for zmin=2, supp=2: 153725\n",
      "Number of itemsets for zmin=2, supp=3: 60391\n",
      "Number of itemsets for zmin=2, supp=4: 29982\n",
      "Number of itemsets for zmin=2, supp=5: 17035\n",
      "Number of itemsets for zmin=2, supp=6: 11130\n",
      "Number of itemsets for zmin=2, supp=7: 7326\n",
      "Number of itemsets for zmin=2, supp=8: 5049\n",
      "Number of itemsets for zmin=2, supp=9: 3670\n",
      "Number of itemsets for zmin=2, supp=10: 2664\n",
      "Number of itemsets for zmin=2, supp=11: 2064\n",
      "Number of itemsets for zmin=2, supp=12: 1613\n",
      "Number of itemsets for zmin=2, supp=13: 1274\n",
      "Number of itemsets for zmin=2, supp=14: 990\n",
      "Number of itemsets for zmin=2, supp=15: 827\n",
      "Number of itemsets for zmin=2, supp=16: 667\n",
      "Number of itemsets for zmin=2, supp=17: 552\n",
      "Number of itemsets for zmin=2, supp=18: 451\n",
      "\n",
      "Number of itemsets for zmin=3, supp=1: 634324\n",
      "Number of itemsets for zmin=3, supp=2: 153572\n",
      "Number of itemsets for zmin=3, supp=3: 60224\n",
      "Number of itemsets for zmin=3, supp=4: 29782\n",
      "Number of itemsets for zmin=3, supp=5: 16809\n",
      "Number of itemsets for zmin=3, supp=6: 10898\n",
      "Number of itemsets for zmin=3, supp=7: 7106\n",
      "Number of itemsets for zmin=3, supp=8: 4844\n",
      "Number of itemsets for zmin=3, supp=9: 3478\n",
      "Number of itemsets for zmin=3, supp=10: 2491\n",
      "Number of itemsets for zmin=3, supp=11: 1883\n",
      "Number of itemsets for zmin=3, supp=12: 1441\n",
      "Number of itemsets for zmin=3, supp=13: 1118\n",
      "Number of itemsets for zmin=3, supp=14: 850\n",
      "Number of itemsets for zmin=3, supp=15: 683\n",
      "Number of itemsets for zmin=3, supp=16: 547\n",
      "Number of itemsets for zmin=3, supp=17: 442\n",
      "Number of itemsets for zmin=3, supp=18: 348\n",
      "\n",
      "Number of itemsets for zmin=4, supp=1: 631617\n",
      "Number of itemsets for zmin=4, supp=2: 150245\n",
      "Number of itemsets for zmin=4, supp=3: 57116\n",
      "Number of itemsets for zmin=4, supp=4: 27158\n",
      "Number of itemsets for zmin=4, supp=5: 14684\n",
      "Number of itemsets for zmin=4, supp=6: 9189\n",
      "Number of itemsets for zmin=4, supp=7: 5669\n",
      "Number of itemsets for zmin=4, supp=8: 3676\n",
      "Number of itemsets for zmin=4, supp=9: 2446\n",
      "Number of itemsets for zmin=4, supp=10: 1654\n",
      "Number of itemsets for zmin=4, supp=11: 1187\n",
      "Number of itemsets for zmin=4, supp=12: 887\n",
      "Number of itemsets for zmin=4, supp=13: 642\n",
      "Number of itemsets for zmin=4, supp=14: 452\n",
      "Number of itemsets for zmin=4, supp=15: 328\n",
      "Number of itemsets for zmin=4, supp=16: 245\n",
      "Number of itemsets for zmin=4, supp=17: 194\n",
      "Number of itemsets for zmin=4, supp=18: 139\n",
      "\n",
      "Number of itemsets for zmin=5, supp=1: 604518\n",
      "Number of itemsets for zmin=5, supp=2: 129854\n",
      "Number of itemsets for zmin=5, supp=3: 43866\n",
      "Number of itemsets for zmin=5, supp=4: 18637\n",
      "Number of itemsets for zmin=5, supp=5: 9162\n",
      "Number of itemsets for zmin=5, supp=6: 4952\n",
      "Number of itemsets for zmin=5, supp=7: 2752\n",
      "Number of itemsets for zmin=5, supp=8: 1652\n",
      "Number of itemsets for zmin=5, supp=9: 941\n",
      "Number of itemsets for zmin=5, supp=10: 537\n",
      "Number of itemsets for zmin=5, supp=11: 345\n",
      "Number of itemsets for zmin=5, supp=12: 228\n",
      "Number of itemsets for zmin=5, supp=13: 164\n",
      "Number of itemsets for zmin=5, supp=14: 101\n",
      "Number of itemsets for zmin=5, supp=15: 62\n",
      "Number of itemsets for zmin=5, supp=16: 35\n",
      "Number of itemsets for zmin=5, supp=17: 15\n",
      "Number of itemsets for zmin=5, supp=18: 10\n",
      "\n",
      "Number of itemsets for zmin=6, supp=1: 501354\n",
      "Number of itemsets for zmin=6, supp=2: 82766\n",
      "Number of itemsets for zmin=6, supp=3: 22342\n",
      "Number of itemsets for zmin=6, supp=4: 7461\n",
      "Number of itemsets for zmin=6, supp=5: 3020\n",
      "Number of itemsets for zmin=6, supp=6: 1370\n",
      "Number of itemsets for zmin=6, supp=7: 575\n",
      "Number of itemsets for zmin=6, supp=8: 296\n",
      "Number of itemsets for zmin=6, supp=9: 135\n",
      "Number of itemsets for zmin=6, supp=10: 60\n",
      "Number of itemsets for zmin=6, supp=11: 22\n",
      "Number of itemsets for zmin=6, supp=12: 8\n",
      "\n",
      "Number of itemsets for zmin=7, supp=1: 310349\n",
      "Number of itemsets for zmin=7, supp=2: 32797\n",
      "Number of itemsets for zmin=7, supp=3: 6036\n",
      "Number of itemsets for zmin=7, supp=4: 1398\n",
      "Number of itemsets for zmin=7, supp=5: 398\n",
      "Number of itemsets for zmin=7, supp=6: 125\n",
      "Number of itemsets for zmin=7, supp=7: 35\n",
      "Number of itemsets for zmin=7, supp=8: 2\n",
      "\n",
      "Number of itemsets for zmin=8, supp=1: 127585\n",
      "Number of itemsets for zmin=8, supp=2: 6577\n",
      "Number of itemsets for zmin=8, supp=3: 660\n",
      "Number of itemsets for zmin=8, supp=4: 81\n",
      "Number of itemsets for zmin=8, supp=5: 2\n",
      "\n",
      "Number of itemsets for zmin=9, supp=1: 30974\n",
      "Number of itemsets for zmin=9, supp=2: 540\n",
      "Number of itemsets for zmin=9, supp=3: 13\n",
      "\n",
      "Number of itemsets for zmin=10, supp=1: 3757\n",
      "Number of itemsets for zmin=10, supp=2: 12\n",
      "\n",
      "Number of itemsets for zmin=11, supp=1: 203\n",
      "\n",
      "Number of itemsets for zmin=12, supp=1: 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zmin_range = range(2, len(df_converted.columns))  # k = 2,...,n_features \n",
    "support_range = range(1, 19)\n",
    "most_specific_frequent_itemset = {}\n",
    "\n",
    "for zmin in zmin_range:\n",
    "    for supp in support_range:\n",
    "        itemsets = apriori(df_db, supp=supp, zmin=zmin, target='m', report='as')  \n",
    "        if len(itemsets) > 0:\n",
    "            print('Number of itemsets for zmin=%s, supp=%s:' % (zmin, supp), len(itemsets))\n",
    "            for itemset in itemsets:\n",
    "                if \"Yes_Attrition\" in itemset[0]:\n",
    "                    most_specific_frequent_itemset = \\\n",
    "                        check_presence_in_most_specific_frequent_itemset(most_specific_frequent_itemset, itemset)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10295\n"
     ]
    }
   ],
   "source": [
    "print(len(most_specific_frequent_itemset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Checking if dict has duplicates </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_most_specific_frequent_itemset = {}\n",
    "index = 0\n",
    "for i in most_specific_frequent_itemset.keys():\n",
    "    duplicate_flag = 0\n",
    "    list(most_specific_frequent_itemset.keys())\n",
    "    for j in most_specific_frequent_itemset.keys():\n",
    "        if set(most_specific_frequent_itemset[i][\"itemset\"]).issubset(set(most_specific_frequent_itemset[j][\"itemset\"])):\n",
    "            duplicate_flag += 1\n",
    "    if duplicate_flag == 1:\n",
    "        new_most_specific_frequent_itemset[str(index)] = {}\n",
    "        new_most_specific_frequent_itemset[str(index)][\"itemset\"] = most_specific_frequent_itemset[i][\"itemset\"]\n",
    "        new_most_specific_frequent_itemset[str(index)][\"support_count\"] = most_specific_frequent_itemset[i][\"support_count\"]\n",
    "        new_most_specific_frequent_itemset[str(index)][\"support\"] = most_specific_frequent_itemset[i][\"support\"]\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10295"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_most_specific_frequent_itemset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_specific_frequent_itemset = new_most_specific_frequent_itemset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10295"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(most_specific_frequent_itemset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Save dict on file </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('kmeans_maximal_yes_attrition_super_itemsets.pickle', 'wb') as handle:\n",
    "    pickle.dump(most_specific_frequent_itemset, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Extract info about dict </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{9, 10, 11, 12, 13}\n"
     ]
    }
   ],
   "source": [
    "# support_count\n",
    "support_count_set = set()\n",
    "for key, value in most_specific_frequent_itemset.items():\n",
    "    support_count = most_specific_frequent_itemset[str(key)][\"support_count\"]\n",
    "    support_count_set.add(support_count)\n",
    "print(support_count_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'9': 7431, '10': 2281, '11': 505, '12': 72, '13': 6}\n"
     ]
    }
   ],
   "source": [
    "# support_count's # of itemsets\n",
    "support_info = {}\n",
    "for elem in support_count_set:\n",
    "    support_info[str(elem)] = 0\n",
    "for key, value in most_specific_frequent_itemset.items():\n",
    "    support_count = most_specific_frequent_itemset[str(key)][\"support_count\"]\n",
    "    support_info[str(support_count)] += 1\n",
    "\n",
    "print(support_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'9': {'4': 816, '5': 2239, '3': 96, '6': 2593, '7': 1350, '2': 4, '8': 298, '9': 35}, '10': {'5': 644, '3': 31, '7': 426, '6': 812, '4': 263, '8': 97, '9': 8}, '11': {'4': 55, '5': 130, '6': 189, '3': 5, '7': 102, '8': 20, '9': 4}, '12': {'5': 15, '7': 19, '6': 32, '4': 3, '9': 1, '8': 2}, '13': {'7': 1, '6': 5}}\n"
     ]
    }
   ],
   "source": [
    "# support_count's zmin\n",
    "support_zmin_info = {}\n",
    "for elem in support_count_set:\n",
    "    support_zmin_info[str(elem)] = {}\n",
    "for key, value in most_specific_frequent_itemset.items():\n",
    "    zmin = len(most_specific_frequent_itemset[str(key)][\"itemset\"])\n",
    "    support_count = most_specific_frequent_itemset[str(key)][\"support_count\"]\n",
    "    try:\n",
    "        support_zmin_info[str(support_count)][str(zmin)] += 1\n",
    "    except KeyError:\n",
    "        support_zmin_info[str(support_count)][str(zmin)] = 1\n",
    "\n",
    "print(support_zmin_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
