{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data mining project - 2020/21</b><br>\n",
    "<b>Author</b>: [Alexandra Bradan](https://github.com/alexandrabradan)<br>\n",
    "<b>Python version</b>: 3.x<br>\n",
    "<b>Last update: 07/01/2021<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# general libraries\n",
    "import sys\n",
    "import math\n",
    "import operator\n",
    "import itertools\n",
    "import pydotplus\n",
    "import collections\n",
    "import missingno as msno\n",
    "from pylab import MaxNLocator\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from IPython.display import Image\n",
    "\n",
    "# pandas libraries\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "# visualisation libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "# numpy libraries\n",
    "import numpy as np\n",
    "from numpy import std\n",
    "from numpy import mean\n",
    "from numpy import arange\n",
    "from numpy import unique\n",
    "from numpy import argmax\n",
    "from numpy import percentile\n",
    "\n",
    "# scipy libraries\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import kstest\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "# sklearn libraries\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.experimental import enable_iterative_imputer  # explicitly require this experimental feature\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from imblearn.pipeline import make_pipeline as imbmake_pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, fbeta_score, recall_score, precision_score, classification_report, roc_auc_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../../../data/\"\n",
    "plot_directory = \"../../../plots/DataUnderstanding/\"\n",
    "TR_cleaned_file = data_directory + \"Discretized_One_Hot_Encoding_Train_HR_Employee_Attrition.csv\"\n",
    "TS_file = data_directory + \"Discretized_One_Hot_Encoding_Test_HR_Employee_Attrition.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv(TR_cleaned_file, sep=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts = pd.read_csv(TS_file , sep=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(883, 102)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219, 98)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Split dataset in Training and Test set </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(883, 95)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column_name in df_cleaned.columns:\n",
    "    if \"OverallSatisfaction\" in column_name:\n",
    "        del df_cleaned[column_name]\n",
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219, 92)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column_name in df_ts.columns:\n",
    "    if \"OverallSatisfaction\" in column_name:\n",
    "        del df_ts[column_name]\n",
    "df_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(883, 94)\n"
     ]
    }
   ],
   "source": [
    "y = df_cleaned['Attrition']\n",
    "df1 = df_cleaned.copy()\n",
    "X = df1.drop('Attrition', axis=1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Class=0 : 730/883 (82.7%)\n",
      "> Class=1 : 153/883 (17.3%)\n"
     ]
    }
   ],
   "source": [
    "# summarize dataset\n",
    "classes = unique(y)\n",
    "total = len(y)\n",
    "for c in classes:\n",
    "    n_examples = len(y[y==c])\n",
    "    percent = n_examples / total * 100\n",
    "    print('> Class=%d : %d/%d (%.1f%%)' % (c, n_examples, total, percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Imbalanced target variable </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our training set is highly unbalanced with respect to the target attribute object of the classification (Attrition=Yes: 153 (17.33%), Attrition=No:730 (82.67%)), we try to solve this problem using three resampling techniques: \n",
    "\n",
    "- **StratifiedKFold**: split a dataset randomly, in such a way that maintains the same class distribution in each fold.\n",
    "- **oversampling**: duplicates examples from minority class;  ==> may lead to overfitting\n",
    "- **undersampling**: deletes or filters examples from the majority class.  ==> may lead to underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize, title, cmap):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    # plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Stratified 70-30 holdout </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"X_filtered = X[best_absolute_features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_filtered, y, test_size=0.3, random_state=1, stratify=y)\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 3  # StratifiedKFold splits\n",
    "no_skill = len(y[y==1]) / len(y)  # PR_curve random model AUC\n",
    "sampling_methods_info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, n_top, scoring, scorings):\n",
    "    configurations = {}\n",
    "    c_i = 0\n",
    "    for i in range(1, n_top + 1):\n",
    "        # retrieeve best i-th model's score index\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)  # returns array([score_index])\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean training score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_train_score'][candidate],\n",
    "                  results['std_train_score'][candidate]))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "            configurations[c_i] = results['params'][candidate]\n",
    "            c_i += 1 \n",
    "    return configurations\n",
    "\n",
    "\n",
    "def report_multiple(results, n_top, scoring, scorings):\n",
    "    configurations = {}\n",
    "    c_i = 0\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_' + str(scoring)] == i)\n",
    "        for candidate in candidates:\n",
    "            \"\"\"print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean training score:\", end = '')\n",
    "            for s in scorings:\n",
    "                print( \"   \" + s + \": {0:.3f} (std: {1:.3f})\".format(\n",
    "                      results['mean_train_' + s][candidate],\n",
    "                      results['std_train_' + s][candidate]), end = '')\n",
    "            \n",
    "\n",
    "            print(\"Mean validation score:\", end = '')\n",
    "            for s in scorings:\n",
    "                print( \"   \" + s + \": {0:.3f} (std: {1:.3f})\".format(\n",
    "                      results['mean_test_' + s][candidate],\n",
    "                      results['std_test_' + s][candidate]), end = '')\n",
    "\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\"\"\"\n",
    "            configurations[c_i] = results['params'][candidate]\n",
    "            c_i += 1 \n",
    "    return configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_best_model_info(resampling_label, models_u, roc_auc_models_u_val, precision_recall_auc_models_u_val,\n",
    "                           best_model_index_u, best_ap_model_index_u, models_thresh, best_thresh, \n",
    "                           ap_best_thresh, scoring, refit):\n",
    "    # keep_track_of_best_model_info\n",
    "    sampling_methods_info[resampling_label] = {}\n",
    "    cnf = models_u[best_model_index_u]\n",
    "    sampling_methods_info[resampling_label][\"model_param\"] = {}\n",
    "    sampling_methods_info[resampling_label][\"model_param\"][\"criterion\"] = cnf.criterion\n",
    "    sampling_methods_info[resampling_label][\"model_param\"][\"max_features\"] = cnf.max_features\n",
    "    sampling_methods_info[resampling_label][\"model_param\"][\"max_depth\"] = cnf.max_depth\n",
    "    sampling_methods_info[resampling_label][\"model_param\"][\"min_samples_split\"] = cnf.min_samples_split\n",
    "    sampling_methods_info[resampling_label][\"model_param\"][\"min_samples_leaf\"] = cnf.min_samples_leaf\n",
    "    sampling_methods_info[resampling_label][\"model_param\"][\"class_weight\"] = cnf.class_weight\n",
    "    sampling_methods_info[resampling_label][\"roc_auc_best_model_index\"] = best_model_index_u\n",
    "    sampling_methods_info[resampling_label][\"roc_auc\"] = roc_auc_models_u_val\n",
    "    sampling_methods_info[resampling_label][\"ap_best_model_index\"] = best_ap_model_index_u\n",
    "    sampling_methods_info[resampling_label][\"ap\"] = precision_recall_auc_models_u_val\n",
    "    sampling_methods_info[resampling_label][\"model_threshold\"] = models_thresh\n",
    "    sampling_methods_info[resampling_label][\"roc_auc_threshold\"] = best_thresh\n",
    "    sampling_methods_info[resampling_label][\"pr_ap_threshold\"] = ap_best_thresh\n",
    "    sampling_methods_info[resampling_label][\"scoring\"] = scoring\n",
    "    sampling_methods_info[resampling_label][\"refit\"] = refit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decision_tree_classifier_params(sampler, resampling_label):\n",
    "    param_list = {'criterion': ['gini', 'entropy'],\n",
    "                  'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "                  'max_depth': [None] + list(np.arange(1, len(X.columns) + 1)),\n",
    "                  # 'min_samples_split': [0.1, 0.01, 0.001], # ceil(min_samples_split * n_samples)\n",
    "                  'min_samples_split':[2, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "                  # 'min_samples_leaf': [0.05, 0.005, 0.0005], # ceil(min_samples_split * n_samples)\n",
    "                  'min_samples_leaf':[2, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "                  'class_weight': ['balanced', [{0:1,1:10}, {0:1,1:100}, {0:1,1:1000}],\n",
    "                                   None],  # balanced={0:153,1:883}, None={0:1,1:1} (Cost-sensitive) \n",
    "                 }\n",
    "    new_params = {'decisiontreeclassifier__' + key: param_list[key] for key in param_list}\n",
    "    return sampler, new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_labels(pos_probs, threshold):\n",
    "    # apply threshold to positive probabilities to create labels\n",
    "    return (pos_probs >= threshold).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_thresholds(model):\n",
    "    model.fit(X_train, y_train)\n",
    "    # predict probabilities\n",
    "    yhat = model.predict_proba(X_test)\n",
    "    # keep probabilities for the positive outcome only\n",
    "    probs = yhat[:, 1]\n",
    "    # define thresholds\n",
    "    thresholds = arange(0, 1, 0.001)\n",
    "    # evaluate each threshold\n",
    "    scores = [f1_score(y_test, to_labels(probs, t)) for t in thresholds]\n",
    "    # get best threshold\n",
    "    ix = argmax(scores)\n",
    "    print( 'ModelThreshold=%.3f, F-measure=%.5f ' % (thresholds[ix], scores[ix]))\n",
    "    return thresholds[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_grid_search(sampler, resampling_label, scoring, refit):    \n",
    "\n",
    "    # declare K-Fold validation to use\n",
    "    # declare model to use\n",
    "    # initialize sampler and GridSearchCV's hyperparameters to tune\n",
    "    cv = StratifiedKFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "    model = DecisionTreeClassifier(random_state=42)  #DECISION TREE\n",
    "    sampler, new_params = get_decision_tree_classifier_params(sampler, resampling_label)\n",
    "    \n",
    "    # declare a imbalance Pipeline to use \n",
    "    # (I'm using imblearn.pipeline since it allows to performe both K-Fold (resampling only on TR's folds)\n",
    "    # and model's tuning, at same time)\n",
    "    if sampler != \"\":\n",
    "        imba_pipeline = imbmake_pipeline(sampler, model)\n",
    "    imba_pipeline = imbmake_pipeline(model)\n",
    "\n",
    "    # perform GridSearchCV (hyperparameters tuning)\n",
    "    grid_search = GridSearchCV(imba_pipeline, param_grid=new_params, cv=cv, scoring=scoring, \n",
    "                                 refit=refit, n_jobs=-1, verbose=1, return_train_score=True)\n",
    "    # build inductive model on TR\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    results = grid_search.cv_results_\n",
    "    cnfs = report_multiple(results, n_top=3, scoring=refit, scorings=scoring)\n",
    "    \n",
    "    # train_models:\n",
    "    if sampler != \"\":\n",
    "        x_u_train_resampled, y_u_train_resampled = sampler.fit_resample(X_train, y_train)\n",
    "    else:\n",
    "        x_u_train_resampled, y_u_train_resampled = X_train, y_train\n",
    "        \n",
    "    models_u = []\n",
    "    y_pred_vals_u = []\n",
    "    y_pred_trains_u = []\n",
    "    hyper_ps = grid_search.cv_results_\n",
    "    model_index = 0\n",
    "    for cnf in cnfs.values():\n",
    "        criterion = cnf['decisiontreeclassifier__criterion']\n",
    "        max_features = cnf['decisiontreeclassifier__max_features']\n",
    "        max_depth = cnf['decisiontreeclassifier__max_depth']\n",
    "        min_samples_split = cnf['decisiontreeclassifier__min_samples_split']\n",
    "        min_samples_leaf = cnf['decisiontreeclassifier__min_samples_leaf']\n",
    "        class_weight = cnf['decisiontreeclassifier__class_weight']\n",
    "        clf = DecisionTreeClassifier(criterion=criterion, \n",
    "                                     max_features=max_features,\n",
    "                                     max_depth=max_depth,\n",
    "                                     min_samples_split=min_samples_split, \n",
    "                                     min_samples_leaf=min_samples_leaf,\n",
    "                                     class_weight=class_weight)\n",
    "        clf = clf.fit(x_u_train_resampled, y_u_train_resampled)\n",
    "        models_u.append(clf)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_pred_tr = clf.predict(x_u_train_resampled)\n",
    "        y_pred_vals_u.append(y_pred)\n",
    "        y_pred_trains_u.append(y_pred_tr)\n",
    "\n",
    "        # plot current model's features importance\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(len(clf.feature_importances_)):\n",
    "            if clf.feature_importances_[i] > 0:\n",
    "                x.append(list(X.columns)[i])\n",
    "                y.append(clf.feature_importances_[i])\n",
    "        plt.bar(x, y)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.xlabel(\"Features\")\n",
    "        plt.title(\"Features importance for model %d\" % (model_index))\n",
    "        plt.show()\n",
    "        model_index += 1\n",
    "\n",
    "\n",
    "    # test_models\n",
    "    roc_auc_models_u_val = []  # models' TS ROC_AUC \n",
    "    precision_recall_auc_models_u_val = []  # models' TS PRECISION_RECALL_AUC\n",
    "    for i in range(0, len(cnfs)):\n",
    "        fpr, tpr, thresholds = roc_curve(y_u_train_resampled, y_pred_trains_u[i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        roc_auc = roc_auc_score(y_u_train_resampled, y_pred_trains_u[i], average=\"weighted\")\n",
    "        print(\"model {}\".format(i))\n",
    "        print('Train Accuracy %s' % accuracy_score(y_u_train_resampled, y_pred_trains_u[i]))\n",
    "        print('Train Precision %s' % precision_score(y_u_train_resampled, y_pred_trains_u[i], average=\"weighted\"))\n",
    "        print('Train Recall %s' % recall_score(y_u_train_resampled, y_pred_trains_u[i], average=\"weighted\"))\n",
    "        print('Train F1-score %s' % f1_score(y_u_train_resampled, y_pred_trains_u[i], average=\"weighted\"))\n",
    "        print('Train F2-score %s' % fbeta_score(y_u_train_resampled, y_pred_trains_u[i], average=\"weighted\",\n",
    "                                               beta=2))\n",
    "        print(\"Train roc_auc: {}\".format(roc_auc))\n",
    "        cm = confusion_matrix(y_u_train_resampled, y_pred_trains_u[i])\n",
    "        plot_confusion_matrix(cm, models_u[i].classes_, False, \"TR's confusion matrix for model %d\" %\\\n",
    "                              (i), plt.cm.Blues)\n",
    "        plt.show()\n",
    "\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_vals_u[i], average=\"weighted\")\n",
    "        roc_auc_models_u_val.append(roc_auc)\n",
    "        print(\"Validation roc_auc: {}\".format(roc_auc))\n",
    "        \n",
    "        pr_ap = average_precision_score(y_test, y_pred_vals_u[i], average=\"weighted\")\n",
    "        precision_recall_auc_models_u_val.append(pr_ap)\n",
    "        print(\"Validation precision_recall_ap: {}\".format(pr_ap))\n",
    "        \n",
    "        print('\\nValidation Accuracy %s' % accuracy_score(y_test, y_pred_vals_u[i]))\n",
    "        print('Validation Precision %s' % precision_score(y_test, y_pred_vals_u[i], average=\"weighted\"))\n",
    "        print('Validation Recall %s' % recall_score(y_test, y_pred_vals_u[i], average=\"weighted\"))\n",
    "        print('Validation F1-score %s' % f1_score(y_test, y_pred_vals_u[i], average=\"weighted\"))\n",
    "        print('Validation F2-score %s' % fbeta_score(y_test, y_pred_vals_u[i], average=\"weighted\", beta=2))\n",
    "        print(classification_report(y_test, y_pred_vals_u[i]))\n",
    "        cm = confusion_matrix(y_test, y_pred_vals_u[i])\n",
    "        plot_confusion_matrix(cm, models_u[i].classes_, False, \"VS' confusion matrix for model %d\" %\\\n",
    "                              (i), plt.cm.Blues)\n",
    "        plt.show()\n",
    "        print('Parameters model %s and resampling_label=%s: criterion=%s, max_features=%s, max_depth=%s, min_samples_split=%s, min_samples_leaf=%s, class_weight=%s\"' % \n",
    "              (i, resampling_label, models_u[i].criterion, models_u[i].max_features, models_u[i].max_depth, \n",
    "               models_u[i].min_samples_split, models_u[i].min_samples_leaf, models_u[i].class_weight))\n",
    "    \n",
    "    # get_best_model_index based on Precision_Recall_AP\n",
    "    for i in range(0,len(cnfs)):\n",
    "        print(\"model {} - precision_recall_ap: {}\".format(i, precision_recall_auc_models_u_val[i]))\n",
    "    best_pr_ap = max(precision_recall_auc_models_u_val)\n",
    "    best_ap_model_index_u = precision_recall_auc_models_u_val.index(best_pr_ap)\n",
    "    print(\"\\nBEST MODEL INDEX = %d\" % best_ap_model_index_u)\n",
    "\n",
    "        \n",
    "    # get_best_model_index based on ROC_AUC\n",
    "    for i in range(0,len(cnfs)):\n",
    "        print(\"model {} - roc_auc: {}\".format(i, roc_auc_models_u_val[i]))\n",
    "    best_roc_auc = max(roc_auc_models_u_val)\n",
    "    best_model_index_u = roc_auc_models_u_val.index(best_roc_auc)\n",
    "    print(\"\\nBEST MODEL INDEX = %d\" % best_model_index_u)\n",
    "    \n",
    "    models_thresh = get_model_thresholds(models_u[best_model_index_u])\n",
    "        \n",
    "    # draw_best_model_precision_recall_curve\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    precision, recall, ap_thresholds = precision_recall_curve(y_test, y_pred_vals_u[best_model_index_u])\n",
    "    # convert to f-measure\n",
    "    fscore = (2 * precision * recall) / (precision + recall)\n",
    "    # locate the index of the largest f-measure\n",
    "    ix = argmax(fscore)\n",
    "    print( ' Best Threshold=%f, F-measure=%.3f ' % (ap_thresholds[ix], fscore[ix]))\n",
    "    ap_best_thresh = ap_thresholds[ix]\n",
    "    plt.plot(precision, recall, label='PR curve (AP=%0.4f)' %\\\n",
    "             (precision_recall_auc_models_u_val[best_model_index_u]))\n",
    "        \n",
    "    # calculate the no skill line as the proportion of the positive class\n",
    "    # plot the no skill precision-recall curve\n",
    "    plt.plot([0, 1], [no_skill, no_skill], 'k--', color=\"red\", label='Random model') \n",
    "    plt.xlim([0.0, 1])\n",
    "    plt.ylim([0.0, 1])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision') \n",
    "    plt.tick_params(axis='both', which='major')\n",
    "    plt.legend(loc=\"upper right\", fontsize=14, frameon=False)\n",
    "    plt.title(\"Model %d's PR AP(average precision)\" % best_model_index_u)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # draw_best_model_roc_auc\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_vals_u[best_model_index_u])\n",
    "    # Youdenâ€™s J statistic \n",
    "    J = tpr - fpr\n",
    "    # locate best threshold's index\n",
    "    ix = argmax(J)\n",
    "    best_thresh = thresholds[ix]\n",
    "    print( ' Best Threshold=%f ' % (best_thresh))\n",
    "    plt.plot(fpr, tpr, label='ROC curve (AUC=%0.4f)' % (roc_auc_models_u_val[best_model_index_u]))\n",
    "        \n",
    "    plt.plot([0, 1], [0, 1], 'k--', color=\"red\", label='Random model') \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate') \n",
    "    plt.tick_params(axis='both', which='major')\n",
    "    plt.legend(loc=\"lower right\", fontsize=14, frameon=False)\n",
    "    plt.title(\"Model %d's ROC AUC\" % best_model_index_u)\n",
    "    plt.show()\n",
    "    \n",
    "    # keap track of best model's hyperparameters\n",
    "    update_best_model_info(resampling_label, models_u, roc_auc_models_u_val, precision_recall_auc_models_u_val,\\\n",
    "                          best_model_index_u, best_ap_model_index_u,  models_thresh, best_thresh, \\\n",
    "                          ap_best_thresh, scoring, refit)\n",
    "    \n",
    "    return models_u, roc_auc_models_u_val, precision_recall_auc_models_u_val, best_model_index_u,\\\n",
    "                                                                                    best_ap_model_index_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Plain TR</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(1, len(s)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scoring_base = [\"recall\"]\\nscoring_pool = [\"f1\", \"roc_auc\", \"precision\", \"f1_weighted\", \"average_precision\", \"roc_auc_ovo_weighted\"] \\nrefit_pool = [\"recall\", \"f1\", \"roc_auc\", \"f1_weighted\", \"average_precision\", \"roc_auc_ovo_weighted\"]'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"scoring_base = [\"recall\"]\n",
    "scoring_pool = [\"f1\", \"roc_auc\", \"precision\", \"f1_weighted\", \"average_precision\", \"roc_auc_ovo_weighted\"] \n",
    "refit_pool = [\"recall\", \"f1\", \"roc_auc\", \"f1_weighted\", \"average_precision\", \"roc_auc_ovo_weighted\"]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_base = [\"recall\"]\n",
    "scoring_pool = [\"roc_auc\", \"precision\", \"f1\", \"average_precision\"] \n",
    "refit_pool = [\"recall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_sets = set()\n",
    "for s in scoring_pool:\n",
    "    # scoring = scoring_base + list(powerset(scoring_pool))\n",
    "    tmp_list = list(powerset(scoring_pool))\n",
    "    max_set_size = len(scoring_pool)\n",
    "    for tmp_tuple in tmp_list:\n",
    "        tmp_tuple += (scoring_base[0],) \n",
    "        scoring_sets.add(tmp_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scoring in scoring_sets:\n",
    "    scoring = list(scoring) \n",
    "    for refit in refit_pool:\n",
    "        resampling_label = \"scoring=\" + str(scoring) + \"refit=\" + str(refit)\n",
    "        print(resampling_label)\n",
    "        models_u, roc_auc_models_u_val, precision_recall_auc_models_u_val, best_model_index_u, best_ap_model_index_u  = \\\n",
    "                                                        my_grid_search(\"\", resampling_label, scoring, \"recall\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_roc_auc_key = \"\"\n",
    "max_roc_auc_mode_index = -1\n",
    "max_roc_auc_value = -1\n",
    "\n",
    "max_ap_key = \"\"\n",
    "max_ap_model_index = -1\n",
    "max_ap = -1\n",
    "for key, value, in sampling_methods_info.items():\n",
    "    roc_auc_best_model_index = sampling_methods_info[key][\"roc_auc_best_model_index\"]\n",
    "    if sampling_methods_info[key][\"roc_auc\"][roc_auc_best_model_index] >= max_roc_auc_value:\n",
    "        max_roc_auc_value = sampling_methods_info[key][\"roc_auc\"][roc_auc_best_model_index]\n",
    "        max_roc_auc_mode_index = roc_auc_best_model_index\n",
    "        max_roc_auc_key = key\n",
    "        \n",
    "    ap_best_model_index = sampling_methods_info[key][\"ap_best_model_index\"]\n",
    "    if sampling_methods_info[key][\"ap\"][ap_best_model_index] >= max_ap:\n",
    "        max_ap = sampling_methods_info[key][\"ap\"][ap_best_model_index]\n",
    "        max_ap_model_index = ap_best_model_index\n",
    "        max_ap_key = key\n",
    "        \n",
    "print(\"max_roc_auc_key\", max_roc_auc_key)\n",
    "print(\"max_roc_auc_mode_index\", max_roc_auc_mode_index)\n",
    "print(\"max_roc_auc\", max_roc_auc_value)\n",
    "print()\n",
    "print(\"max_ap_model_index\", max_ap_model_index)\n",
    "print(\"max_ap_key\", max_ap_key)\n",
    "print(\"max_ap\", max_ap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
