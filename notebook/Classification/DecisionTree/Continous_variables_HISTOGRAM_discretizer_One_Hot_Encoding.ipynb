{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data mining project - 2020/21</b><br>\n",
    "<b>Author</b>: [Alexandra Bradan](https://github.com/alexandrabradan)<br>\n",
    "<b>Python version</b>: 3.x<br>\n",
    "<b>Last update: 20/11/2020<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# general libraries\n",
    "import sys\n",
    "import math\n",
    "import operator\n",
    "import itertools\n",
    "import pydotplus\n",
    "import collections\n",
    "import missingno as msno\n",
    "from pylab import MaxNLocator\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from IPython.display import Image\n",
    "\n",
    "# pandas libraries\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "# visualisation libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "# numpy libraries\n",
    "import numpy as np\n",
    "from numpy import std\n",
    "from numpy import mean\n",
    "from numpy import arange\n",
    "from numpy import unique\n",
    "from numpy import percentile\n",
    "\n",
    "# scipy libraries\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import kstest\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "# sklearn libraries\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.experimental import enable_iterative_imputer  # explicitly require this experimental feature\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from imblearn.pipeline import make_pipeline as imbmake_pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, fbeta_score, recall_score, precision_score, classification_report, roc_auc_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../../../data/\"\n",
    "plot_directory = \"../../../plots/DataUnderstanding/\"\n",
    "TR_file = data_directory + \"Train_HR_Employee_Attrition.csv\"\n",
    "TR_cleaned_file = data_directory + \"Cleaned_Train_HR_Employee_Attrition.csv\"\n",
    "TS_file = data_directory + \"One_Hot_Encoding_Test_HR_Employee_Attrition.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv(TR_cleaned_file, sep=\",\") \n",
    "df_ts = pd.read_csv(TS_file, sep=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 883 entries, 0 to 882\n",
      "Data columns (total 36 columns):\n",
      " #   Column                                  Non-Null Count  Dtype  \n",
      "---  ------                                  --------------  -----  \n",
      " 0   Age                                     883 non-null    int64  \n",
      " 1   Attrition                               883 non-null    int64  \n",
      " 2   BusinessTravel_Non-Travel               883 non-null    int64  \n",
      " 3   BusinessTravel_Travel_Rarely            883 non-null    int64  \n",
      " 4   BusinessTravel_Travel_Frequently        883 non-null    int64  \n",
      " 5   DistanceFromHome                        883 non-null    int64  \n",
      " 6   Education                               883 non-null    int64  \n",
      " 7   EnvironmentSatisfaction                 883 non-null    int64  \n",
      " 8   Gender                                  883 non-null    int64  \n",
      " 9   JobInvolvement                          883 non-null    int64  \n",
      " 10  JobLevel                                883 non-null    int64  \n",
      " 11  JobRole_Healthcare_Representative       883 non-null    int64  \n",
      " 12  JobRole_Human_Resources                 883 non-null    int64  \n",
      " 13  JobRole_Laboratory_Technician           883 non-null    int64  \n",
      " 14  JobRole_Manager_Research_&_Development  883 non-null    int64  \n",
      " 15  JobRole_Manager_Sales                   883 non-null    int64  \n",
      " 16  JobRole_Manager_Human_Resources         883 non-null    int64  \n",
      " 17  JobRole_Manufacturing_Director          883 non-null    int64  \n",
      " 18  JobRole_Research_Director               883 non-null    int64  \n",
      " 19  JobRole_Research_Scientist              883 non-null    int64  \n",
      " 20  JobRole_Sales_Executive                 883 non-null    int64  \n",
      " 21  JobRole_Sales_Representative            883 non-null    int64  \n",
      " 22  JobSatisfaction                         883 non-null    int64  \n",
      " 23  MonthlyIncome                           883 non-null    int64  \n",
      " 24  NumCompaniesWorked                      883 non-null    int64  \n",
      " 25  OverTime                                883 non-null    int64  \n",
      " 26  PercentSalaryHike                       883 non-null    int64  \n",
      " 27  RelationshipSatisfaction                883 non-null    int64  \n",
      " 28  StockOptionLevel                        883 non-null    int64  \n",
      " 29  TrainingTimesLastYear                   883 non-null    int64  \n",
      " 30  WorkLifeBalance                         883 non-null    int64  \n",
      " 31  YearsAtCompany                          883 non-null    int64  \n",
      " 32  YearsInCurrentRole                      883 non-null    int64  \n",
      " 33  MonthlyHours                            883 non-null    float64\n",
      " 34  TaxRate                                 883 non-null    float64\n",
      " 35  OverallSatisfaction                     883 non-null    float64\n",
      "dtypes: float64(3), int64(33)\n",
      "memory usage: 248.5 KB\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(883, 36)\n",
      "(219, 37)\n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned.shape)\n",
    "print(df_ts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Discretisation approach </h2> \n",
    "Approaches to transform continuous variables into discrete ones. This process is also known as <b>binning</b>, with each bin being each interval. Discretization methods fall into 2 categories: \n",
    "\n",
    "- supervised: do not use any information, other than the variable distribution, to create the contiguous bins in which the values will be placed;\n",
    "- unsupervised: typically use target information in order to create bins or intervals.\n",
    "\n",
    "Since we are dealying with DT it is natural to use a **supervised discretisation method** with them:\n",
    "\n",
    "<u>Step 1</u>: First it trains a decision tree of limited depth (2, 3 or 4) using the variable we want to discretize to predict the target;\n",
    "\n",
    "<u>Step 2</u>: The original variable values are then replaced by the probability returned by the tree. The probability is the same for all the observations within a single bin, thus replacing by the probability is equivalent to grouping the observations within the cut-off decided by the decision tree.\n",
    "\n",
    "**Advantages** :\n",
    "- The probabilistic predictions returned decision tree are monotonically related to the target.\n",
    "- The new bins show decreased entropy, this is the observations within each bucket/bin are more similar to themselves than to those of other buckets/bins.\n",
    "- The tree finds the bins automatically.\n",
    "\n",
    "**Disadvantages**:\n",
    "- It may cause over-fitting\n",
    "- More importantly, some tuning of tree parameters might need to be done to obtain the optimal splits (e.g., depth, the minimum number of samples in one partition, the maximum number of partitions, and a minimum information gain). This it can be time-consuming.\n",
    "\n",
    "<u>Features to discretize</u>:\n",
    "- Age\n",
    "- DistanceFromHome\n",
    "- YearsAtCompany\n",
    "- YearsInCurrentRole\n",
    "- NumCompaniesWorked\n",
    "- MonthlyIncome\n",
    "- MonthlyHours\n",
    "\n",
    "- PercentSalaryHike\n",
    "- TaxRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training discretisation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_cleaned.copy()\n",
    "y_train = df_cleaned['Attrition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_based_on_histogram_distribution(curr_column, bins, labels):\n",
    "    print(\"%s max_train\" %curr_column, df_cleaned[curr_column].max(), \"%s min_train\" % curr_column, df_cleaned[curr_column].min())\n",
    "    print(\"%s max_test\" %curr_column, df_ts[curr_column].max(), \"%s min_test\" % curr_column, df_ts[curr_column].min())\n",
    "    print(pd.cut(df_cleaned[curr_column], bins, labels=labels, include_lowest=True, right=False).unique())\n",
    "    df_cleaned[curr_column] = pd.cut(df_cleaned[curr_column], bins, labels=labels, include_lowest=True, right=False).astype(int)\n",
    "    print(pd.cut(df_ts[curr_column], bins, labels=labels, include_lowest=True, right=False).unique())\n",
    "    df_ts[curr_column] = pd.cut(df_ts[curr_column], bins, labels=labels, include_lowest=True, right=False).astype(int)\n",
    "    print(\"%s train_unique\" % curr_column, sorted(df_cleaned[curr_column].unique()))\n",
    "    print(\"%s test_unique\" % curr_column, sorted(df_ts[curr_column].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Age </h6>\n",
    "Build a classification tree using the Age to predict Attrition in order to discretise the age variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age max_train 60 Age min_train 18\n",
      "Age max_test 58 Age min_test 22\n",
      "[5, 3, 2, 4, 6, 1]\n",
      "Categories (6, int64): [1 < 2 < 3 < 4 < 5 < 6]\n",
      "[2, 3, 5, 4]\n",
      "Categories (4, int64): [2 < 3 < 4 < 5]\n",
      "Age train_unique [1, 2, 3, 4, 5, 6]\n",
      "Age test_unique [2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "bins = list(range(10, 71, 10))\n",
    "labels= list(range(1, len(bins)))\n",
    "discretize_based_on_histogram_distribution(\"Age\", bins, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>DistanceFromHome </h6>\n",
    "Build a classification tree using the variable to predict Attrition in order to discretise it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistanceFromHome max_train 29 DistanceFromHome min_train 1\n",
      "DistanceFromHome max_test 29 DistanceFromHome min_test 1\n",
      "[2, 1, 4, 6, 3, 5]\n",
      "Categories (6, int64): [1 < 2 < 3 < 4 < 5 < 6]\n",
      "[4, 1, 2, 3, 5, 6]\n",
      "Categories (6, int64): [1 < 2 < 3 < 4 < 5 < 6]\n",
      "DistanceFromHome train_unique [1, 2, 3, 4, 5, 6]\n",
      "DistanceFromHome test_unique [1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "bins = list(range(0, 31, 5))\n",
    "labels= list(range(1, len(bins)))\n",
    "discretize_based_on_histogram_distribution(\"DistanceFromHome\", bins, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> YearsAtCompany </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped rows = \t0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(219, 37)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop_indexes = df_ts.index[df_ts[\"YearsAtCompany\"] > 20]\n",
    "df_ts.drop(list(to_drop_indexes), axis=0, inplace=True)\n",
    "df_ts.reset_index(drop=True, inplace=True)\n",
    "print(\"dropped rows = \", len(to_drop_indexes), sep=\"\\t\")\n",
    "\n",
    "df_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YearsAtCompany max_train 20 YearsAtCompany min_train 0\n",
      "YearsAtCompany max_test 20 YearsAtCompany min_test 0\n",
      "[2, 1, 3, 4, 5]\n",
      "Categories (5, int64): [1 < 2 < 3 < 4 < 5]\n",
      "[1, 2, 3, 5, 4]\n",
      "Categories (5, int64): [1 < 2 < 3 < 4 < 5]\n",
      "YearsAtCompany train_unique [1, 2, 3, 4, 5]\n",
      "YearsAtCompany test_unique [1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "bins = list(range(0, 26, 5), )\n",
    "labels= list(range(1, len(bins)))\n",
    "discretize_based_on_histogram_distribution(\"YearsAtCompany\", bins, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> YearsInCurrentRole </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped rows = \t0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(219, 37)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop_indexes = df_ts.index[df_ts[\"YearsInCurrentRole\"] > 16]\n",
    "df_ts.drop(list(to_drop_indexes), axis=0, inplace=True)\n",
    "df_ts.reset_index(drop=True, inplace=True)\n",
    "print(\"dropped rows = \", len(to_drop_indexes), sep=\"\\t\")\n",
    "\n",
    "df_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YearsInCurrentRole max_train 16 YearsInCurrentRole min_train 0\n",
      "YearsInCurrentRole max_test 15 YearsInCurrentRole min_test 0\n",
      "[2, 1, 3, 4]\n",
      "Categories (4, int64): [1 < 2 < 3 < 4]\n",
      "[1, 3, 2, 4]\n",
      "Categories (4, int64): [1 < 2 < 3 < 4]\n",
      "YearsInCurrentRole train_unique [1, 2, 3, 4]\n",
      "YearsInCurrentRole test_unique [1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "bins = list(range(0, 21, 5))\n",
    "labels= list(range(1, len(bins)))\n",
    "discretize_based_on_histogram_distribution(\"YearsInCurrentRole\", bins, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> NumCompaniesWorked </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumCompaniesWorked max_train 9 NumCompaniesWorked min_train 0\n",
      "NumCompaniesWorked max_test 9 NumCompaniesWorked min_test 0\n",
      "[2, 1]\n",
      "Categories (2, int64): [1 < 2]\n",
      "[2, 1]\n",
      "Categories (2, int64): [1 < 2]\n",
      "NumCompaniesWorked train_unique [1, 2]\n",
      "NumCompaniesWorked test_unique [1, 2]\n"
     ]
    }
   ],
   "source": [
    "bins = list(range(0, 11, 5))\n",
    "labels= list(range(1, len(bins)))\n",
    "discretize_based_on_histogram_distribution(\"NumCompaniesWorked\", bins, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumCompaniesWorked is a discretisation candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> MonthlyIncome </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MonthlyIncome max_train 26997 MonthlyIncome min_train 1009\n",
      "MonthlyIncome max_test 25479 MonthlyIncome min_test 1393\n",
      "[4, 2, 3, 1, 6, ..., 5, 11, 10, 8, 9]\n",
      "Length: 11\n",
      "Categories (11, int64): [1 < 2 < 3 < 4 ... 8 < 9 < 10 < 11]\n",
      "[2, 3, 5, 4, 1, ..., 7, 8, 11, 10, 9]\n",
      "Length: 11\n",
      "Categories (11, int64): [1 < 2 < 3 < 4 ... 8 < 9 < 10 < 11]\n",
      "MonthlyIncome train_unique [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "MonthlyIncome test_unique [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n"
     ]
    }
   ],
   "source": [
    "bins = list(range(0, 30000, 2500))\n",
    "labels= list(range(1, len(bins)))\n",
    "discretize_based_on_histogram_distribution(\"MonthlyIncome\", bins, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> MonthlyHours </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped rows = \t0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(219, 37)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop_indexes = df_ts.index[df_ts[\"MonthlyHours\"] > 590.9767441860465]\n",
    "df_ts.drop(list(to_drop_indexes), axis=0, inplace=True)\n",
    "df_ts.reset_index(drop=True, inplace=True)\n",
    "print(\"dropped rows = \", len(to_drop_indexes), sep=\"\\t\")\n",
    "\n",
    "df_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MonthlyHours max_train 590.9767441860465 MonthlyHours min_train 26.04347826086957\n",
      "MonthlyHours max_test 574.7954545454545 MonthlyHours min_test 33.71590909090909\n",
      "[1, 3, 2]\n",
      "Categories (3, int64): [1 < 2 < 3]\n",
      "[2, 3, 1]\n",
      "Categories (3, int64): [1 < 2 < 3]\n",
      "MonthlyHours train_unique [1, 2, 3]\n",
      "MonthlyHours test_unique [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "bins = list(range(0, 601, 200))\n",
    "labels= list(range(1, len(bins)))\n",
    "discretize_based_on_histogram_distribution(\"MonthlyHours\", bins, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> PercentSalaryHike </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PercentSalaryHike max_train 25 PercentSalaryHike min_train 11\n",
      "PercentSalaryHike max_test 25 PercentSalaryHike min_test 11\n",
      "[4, 3, 6, 5]\n",
      "Categories (4, int64): [3 < 4 < 5 < 6]\n",
      "[3, 4, 5, 6]\n",
      "Categories (4, int64): [3 < 4 < 5 < 6]\n",
      "PercentSalaryHike train_unique [3, 4, 5, 6]\n",
      "PercentSalaryHike test_unique [3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "bins = list(range(0, 31, 5))\n",
    "labels= list(range(1, len(bins)))\n",
    "discretize_based_on_histogram_distribution(\"PercentSalaryHike\", bins, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> TaxRate </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bins [0.0, 0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9, 1.0]\n",
      "TaxRate max_train 0.9513959334891722 TaxRate min_train 0.0\n",
      "TaxRate max_test 0.9138676137092978 TaxRate min_test 0.0\n",
      "[4, 5, 8, 9, 3, 6, 1, 7, 2, 10]\n",
      "Categories (10, int64): [1 < 2 < 3 < 4 ... 7 < 8 < 9 < 10]\n",
      "[9, 8, 7, 1, 4, 3, 6, 2, 5, 10]\n",
      "Categories (10, int64): [1 < 2 < 3 < 4 ... 7 < 8 < 9 < 10]\n",
      "TaxRate train_unique [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "TaxRate test_unique [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "bins = list(np.linspace(0, 1, 11))\n",
    "print(\"bins\", bins)\n",
    "labels= list(range(1, len(bins)))\n",
    "discretize_based_on_histogram_distribution(\"TaxRate\", bins, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> OverallSatisfaction </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bins [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n",
      "OverallSatisfaction max_train 4.0 OverallSatisfaction min_train 1.2\n",
      "OverallSatisfaction max_test 3.6 OverallSatisfaction min_test 1.4\n",
      "[6, 7, 5, 8, 3, 4, 9]\n",
      "Categories (7, int64): [3 < 4 < 5 < 6 < 7 < 8 < 9]\n",
      "[5, 7, 6, 8, 4, 3]\n",
      "Categories (6, int64): [3 < 4 < 5 < 6 < 7 < 8]\n",
      "OverallSatisfaction train_unique [3, 4, 5, 6, 7, 8, 9]\n",
      "OverallSatisfaction test_unique [3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "bins = list(np.linspace(0, 5, 11))\n",
    "print(\"bins\", bins)\n",
    "labels= list(range(1, len(bins)))\n",
    "discretize_based_on_histogram_distribution(\"OverallSatisfaction\", bins, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Discretize variables and save them on new file </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_probabilities_to_increasing_integers(var_tree):\n",
    "    values = sorted(list(X_train[var_tree].unique()))\n",
    "    values_map = {}\n",
    "    for i in range(1, len(values) + 1):\n",
    "        values_map[str(values[i-1])] = i\n",
    "    return values_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bin_indeces(var, X, train_or_test_flag):\n",
    "    var_tree = \"%s_tree\" % var\n",
    "    values_map = map_probabilities_to_increasing_integers(var_tree)\n",
    "    bin_edges = [float(x) for x in list(values_map.keys())]\n",
    "    bin_indeces = []\n",
    "    if train_or_test_flag == \"test\":\n",
    "        values = X[var]\n",
    "    else:\n",
    "        values = X[var_tree]\n",
    "    for x in values:\n",
    "        for edge in bin_edges:\n",
    "            if x <= edge:\n",
    "                bin_indeces.append(values_map[str(edge)])\n",
    "                break\n",
    "    return bin_indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_categorical_feature_with_dummy_ones(column_name, categories_list, dummy_features, X):\n",
    "    \"\"\"\n",
    "    Function which replaces the nominal feature passed by argument with dummy ones, \n",
    "    to convert nominal column's M values in M new binary (dummy) features.\n",
    "    \"\"\"\n",
    "    # retrive nominal feature's index. It is used to know where to insert the new M binary features\n",
    "    index = X.columns.get_loc(column_name)\n",
    "    for i in range(0, dummy_features.shape[1]):\n",
    "        index += 1\n",
    "        X.insert(index, column_name + \"_\" + str(categories_list[i]), \n",
    "                                                              dummy_features[:, i].todense().astype(int), True)\n",
    "    # remove categorical feature\n",
    "    del X[column_name]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_one_encoding(column_name, X):\n",
    "    unique = list(X[column_name].unique())\n",
    "    categories_list = sorted(unique)\n",
    "    encoder = OneHotEncoder(categories=[categories_list])   # excplict force encoding order\n",
    "    # fit and transform model on data\n",
    "    dummy_features = encoder.fit_transform(X[column_name].values.reshape(-1,1))\n",
    "    # add dummy features to dataset, replacing categorical feature\n",
    "    return replace_categorical_feature_with_dummy_ones(column_name, categories_list, dummy_features, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      2\n",
      "1      1\n",
      "2      4\n",
      "3      2\n",
      "4      2\n",
      "      ..\n",
      "878    3\n",
      "879    4\n",
      "880    2\n",
      "881    1\n",
      "882    1\n",
      "Name: DistanceFromHome, Length: 883, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned[\"DistanceFromHome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistanceFromHome\n",
      "YearsAtCompany\n",
      "YearsInCurrentRole\n",
      "NumCompaniesWorked\n",
      "MonthlyIncome\n",
      "PercentSalaryHike\n",
      "TaxRate\n",
      "Age\n",
      "MonthlyHours\n",
      "OverallSatisfaction\n"
     ]
    }
   ],
   "source": [
    "discrete_variables = [\"DistanceFromHome\", \"YearsAtCompany\", \"YearsInCurrentRole\", \"NumCompaniesWorked\",\n",
    "                      \"MonthlyIncome\", \"PercentSalaryHike\", \"TaxRate\", \"Age\", \"MonthlyHours\", \n",
    "                      \"OverallSatisfaction\"]\n",
    "for var in discrete_variables:\n",
    "    print(var)\n",
    "    df_cleaned = perform_one_encoding(var, df_cleaned)\n",
    "    df_ts = perform_one_encoding(var, df_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_variables = [\"Education\", \"JobLevel\", \"EnvironmentSatisfaction\", \"JobInvolvement\", \"JobSatisfaction\",\n",
    "                      \"RelationshipSatisfaction\", \"WorkLifeBalance\", \"StockOptionLevel\", \"TrainingTimesLastYear\"]\n",
    "for var in discrete_variables:\n",
    "    df_cleaned = perform_one_encoding(var, df_cleaned)\n",
    "    df_ts = perform_one_encoding(var, df_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in df_cleaned.columns:\n",
    "    if len(sorted(df_cleaned[x].unique())) > 2:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in df_ts.columns:\n",
    "    if len(sorted(df_ts[x].unique())) > 2:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(883, 116)\n",
      "(219, 114)\n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned.shape)\n",
    "print(df_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_cleaned.copy()\n",
    "df2 = df_ts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(883, 116)\n",
      "(219, 114)\n"
     ]
    }
   ],
   "source": [
    "print(df1.shape)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Age_1', 'Age_6', 'OverallSatisfaction_9'}"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df1.columns).difference(set(df2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(data_directory + \"Discretized_HISTOGRAM_One_Hot_Encoding_Train_HR_Employee_Attrition.csv\", index=False, header=True)\n",
    "df2.to_csv(data_directory + \"Discretized_HISTOGRAM_One_Hot_Encoding_Test_HR_Employee_Attrition.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(883, 116)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_discretized = pd.read_csv(data_directory + \"Discretized_HISTOGRAM_One_Hot_Encoding_Train_HR_Employee_Attrition.csv\", sep=\",\") \n",
    "df_discretized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219, 114)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_discretized = pd.read_csv(data_directory + \"Discretized_HISTOGRAM_One_Hot_Encoding_Test_HR_Employee_Attrition.csv\", sep=\",\") \n",
    "df_discretized.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
